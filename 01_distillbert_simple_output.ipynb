{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, how are you?\")).unsqueeze(0)\n",
    "outputs = model(input_ids)\n",
    "print(outputs[\"last_hidden_state\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Size (1): Dies zeigt an, dass Ihre Eingabe nur aus einem einzigen Datensatz (oder Beispiel) besteht. In neuronalen Netzen wird oft eine Gruppe von Daten gleichzeitig verarbeitet (genannt \"Batch\"). Da Sie hier nur einen Satz verarbeiten, ist die Batch-Größe 1.\n",
    "\n",
    "* Anzahl der Tokens (8): Dies ist die Anzahl der Tokens, in die Ihr Eingabetext zerlegt wurde. In Ihrem Fall wird der Satz \"Hello, how are\" vom Tokenizer in 6 Tokens aufgeteilt. Diese 6 Tokens beinhalten auch spezielle Tokens wie [CLS] (am Anfang) und [SEP] (am Ende), die vom BERT-Modell für verschiedene Zwecke verwendet werden. Der Tokenizer fügt diese speziellen Tokens automatisch hinzu.\n",
    "\n",
    "* Embedding-Größe (768): Dies ist die Größe des Embedding-Vektors für jedes Token. DistilBERT, wie viele BERT-Varianten, verwendet 768-dimensionale Vektoren, um die Merkmale jedes Tokens darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
