{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>To Make Female Hearts Flutter in Iraq, Throw a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>British Liberal Democrat Patsy Calton, 56, die...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>Drone smartphone app to help heart attack vict...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0                                     Should I Get Bings          1\n",
       "1          Which TV Female Friend Group Do You Belong In          1\n",
       "2      The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3      This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4      A Couple Did A Stunning Photo Shoot With Their...          1\n",
       "...                                                  ...        ...\n",
       "31995  To Make Female Hearts Flutter in Iraq, Throw a...          0\n",
       "31996  British Liberal Democrat Patsy Calton, 56, die...          0\n",
       "31997  Drone smartphone app to help heart attack vict...          0\n",
       "31998  Netanyahu Urges Pope Benedict, in Israel, to D...          0\n",
       "31999  Computer Makers Prepare to Stake Bigger Claim ...          0\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle=False\n",
    "\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# models: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "path='/kaggle/input/clickbait-dataset/clickbait_data.csv' if kaggle else 'clickbait_data.csv'\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "\n",
    "SEQ_LEN = 30\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text=self.df.iloc[idx]['headline']\n",
    "\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if (len(input_ids)<SEQ_LEN):\n",
    "            input_ids=input_ids+[0]*(SEQ_LEN-len(input_ids))\n",
    "            attention_mask=attention_mask+[0]*(SEQ_LEN-len(attention_mask))\n",
    "        elif (len(input_ids)>SEQ_LEN):\n",
    "            input_ids=input_ids[:SEQ_LEN]\n",
    "            attention_mask=attention_mask[:SEQ_LEN]\n",
    "        label=self.df.iloc[idx]['clickbait']\n",
    "        return torch.tensor(input_ids), torch.tensor(label), torch.tensor(attention_mask)\n",
    "    \n",
    "train_dataset = Dataset(train_df)\n",
    "valid_dataset = Dataset(valid_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "        self.linear = torch.nn.Linear(768, 2)\n",
    "        self.maxpool=torch.nn.MaxPool1d(SEQ_LEN)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        output = self.model(x, attention_mask=attention_mask)[\"last_hidden_state\"]\n",
    "        pooled_output = torch.mean(output, dim=1)\n",
    "        output = self.linear(pooled_output)\n",
    "        return output\n",
    "    \n",
    "my_model=Classifier()\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for xb, yb, att_mask in train_dataloader:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    print(att_mask.shape)\n",
    "    print(my_model(xb, attention_mask=att_mask).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(my_model.parameters(), lr=0.0001)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "import neptune\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, optimizer, loss_fn, batch_size=32):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_fn=loss_fn\n",
    "\n",
    "        self.device=torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.device=torch.device(\"cuda\")\n",
    "        #elif torch.backends.mps.is_available():\n",
    "        #    self.device=torch.device(\"mps\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.run=neptune.init_run(\n",
    "            project=\"bernd.heidemann/clickbait-classification\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n",
    "        )\n",
    "        self.batch_size=batch_size\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        self.run[\"parameters\"] = {\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "        }\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            batch_count=0\n",
    "            print(\"batches: \", len(self.train_dataloader))\n",
    "            for xb, yb, att_mask in self.train_dataloader:\n",
    "                batch_count+=1\n",
    "                print(\"batch: {}\".format(batch_count))\n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                loss=self.loss_fn(pred, yb)\n",
    "                self.run[\"train_loss\"].log(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            self.model.eval()\n",
    "            # log current state to neptune\n",
    "            with torch.no_grad():\n",
    "                valid_loss=0\n",
    "                for xb, yb, att_mask in self.valid_dataloader:\n",
    "                    xb=xb.to(self.device)\n",
    "                    yb=yb.to(self.device)\n",
    "                    att_mask=att_mask.to(self.device)\n",
    "                    pred=self.model(xb, attention_mask=att_mask)\n",
    "                    loss=self.loss_fn(pred, yb)\n",
    "                    valid_loss+=loss.item()\n",
    "                print(\"epoch: {}, valid_loss: {}\".format(epoch, valid_loss/len(self.valid_dataloader)))\n",
    "                self.run[\"valid_loss\"].log(valid_loss/len(self.valid_dataloader))\n",
    "                \n",
    "    def get_accuracy(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct=0\n",
    "            for xb, yb, att_mask in self.valid_dataloader:\n",
    "                xb=xb.to(self.device)\n",
    "                yb=yb.to(self.device)\n",
    "                att_mask=att_mask.to(self.device)\n",
    "                pred=self.model(xb, attention_mask=att_mask)\n",
    "                pred=torch.argmax(pred, dim=1)\n",
    "                correct+=torch.sum(pred==yb).item()\n",
    "            return correct/len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_20891/1450912362.py:18: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  self.run=neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-20\n",
      "batches:  200\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learner\u001b[38;5;241m=\u001b[39mLearner(my_model, optimizer, loss_fn, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, lr, epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(pred, yb)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner=Learner(my_model, optimizer, loss_fn, batch_size=128)\n",
    "learner.fit(lr=0.0001, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98171875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/bernd.heidemann/clickbait-classification/e/CLIC-17/metadata\n"
     ]
    }
   ],
   "source": [
    "learner.run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
